{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1 - Preliminaries #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:34.800788Z",
     "iopub.status.busy": "2022-06-18T06:54:34.800348Z",
     "iopub.status.idle": "2022-06-18T06:54:34.810294Z",
     "shell.execute_reply": "2022-06-18T06:54:34.809514Z",
     "shell.execute_reply.started": "2022-06-18T06:54:34.800753Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:34.990433Z",
     "iopub.status.busy": "2022-06-18T06:54:34.990063Z",
     "iopub.status.idle": "2022-06-18T06:54:34.996974Z",
     "shell.execute_reply": "2022-06-18T06:54:34.995857Z",
     "shell.execute_reply.started": "2022-06-18T06:54:34.990404Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read data\n",
    "    data_dir = Path(\"G:/MLbook/Housing_price_prediction\")\n",
    "    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n",
    "    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n",
    "    # Merge the splits so we can process them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    # Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    df = impute(df)\n",
    "    # Reform splits\n",
    "    df_train = df.loc[df_train.index, :]\n",
    "    df_test = df.loc[df_test.index, :]\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:35.045784Z",
     "iopub.status.busy": "2022-06-18T06:54:35.044813Z",
     "iopub.status.idle": "2022-06-18T06:54:35.078526Z",
     "shell.execute_reply": "2022-06-18T06:54:35.077932Z",
     "shell.execute_reply.started": "2022-06-18T06:54:35.045745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VinylSd', 'MetalSd', 'Wd Shng', 'HdBoard', 'Plywood', 'Wd Sdng',\n",
       "       'CmentBd', 'BrkFace', 'Stucco', 'AsbShng', 'Brk Cmn', 'ImStucc',\n",
       "       'AsphShn', 'Stone', 'Other', 'CBlock'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(\"G:/MLbook/Housing_price_prediction\")\n",
    "df = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n",
    "\n",
    "df.Exterior2nd.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:35.226208Z",
     "iopub.status.busy": "2022-06-18T06:54:35.225545Z",
     "iopub.status.idle": "2022-06-18T06:54:35.231795Z",
     "shell.execute_reply": "2022-06-18T06:54:35.231185Z",
     "shell.execute_reply.started": "2022-06-18T06:54:35.226171Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "    # Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "    # with the year the house was built\n",
    "    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n",
    "    # Names beginning with numbers are awkward to work with\n",
    "    df.rename(columns={\n",
    "        \"1stFlrSF\": \"FirstFlrSF\",\n",
    "        \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "        \"3SsnPorch\": \"Threeseasonporch\",\n",
    "    }, inplace=True,\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Statistical Data Type ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:35.447927Z",
     "iopub.status.busy": "2022-06-18T06:54:35.446644Z",
     "iopub.status.idle": "2022-06-18T06:54:35.468479Z",
     "shell.execute_reply": "2022-06-18T06:54:35.467474Z",
     "shell.execute_reply.started": "2022-06-18T06:54:35.447851Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The numeric features are already encoded correctly (`float` for\n",
    "# continuous, `int` for discrete), but the categoricals we'll need to\n",
    "# do ourselves. Note in particular, that the `MSSubClass` feature is\n",
    "# read as an `int` type, but is actually a (nominative) categorical.\n",
    "\n",
    "# The nominative (unordered) categorical features\n",
    "features_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n",
    "\n",
    "\n",
    "# The ordinal (ordered) categorical features \n",
    "\n",
    "# Pandas calls the categories \"levels\"\n",
    "five_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "ten_levels = list(range(10))\n",
    "\n",
    "ordered_levels = {\n",
    "    \"OverallQual\": ten_levels,\n",
    "    \"OverallCond\": ten_levels,\n",
    "    \"ExterQual\": five_levels,\n",
    "    \"ExterCond\": five_levels,\n",
    "    \"BsmtQual\": five_levels,\n",
    "    \"BsmtCond\": five_levels,\n",
    "    \"HeatingQC\": five_levels,\n",
    "    \"KitchenQual\": five_levels,\n",
    "    \"FireplaceQu\": five_levels,\n",
    "    \"GarageQual\": five_levels,\n",
    "    \"GarageCond\": five_levels,\n",
    "    \"PoolQC\": five_levels,\n",
    "    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n",
    "    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "}\n",
    "\n",
    "# Add a None level for missing values\n",
    "ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "\n",
    "def encode(df):\n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        # Add a None category for missing values\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name].cat.add_categories(\"None\", inplace=True)\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values ###\n",
    "\n",
    "Handling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:35.902129Z",
     "iopub.status.busy": "2022-06-18T06:54:35.901256Z",
     "iopub.status.idle": "2022-06-18T06:54:35.908909Z",
     "shell.execute_reply": "2022-06-18T06:54:35.908028Z",
     "shell.execute_reply.started": "2022-06-18T06:54:35.902079Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(0)\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(\"None\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:35.912451Z",
     "iopub.status.busy": "2022-06-18T06:54:35.91128Z",
     "iopub.status.idle": "2022-06-18T06:54:36.110734Z",
     "shell.execute_reply": "2022-06-18T06:54:36.109975Z",
     "shell.execute_reply.started": "2022-06-18T06:54:35.912401Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:36.113123Z",
     "iopub.status.busy": "2022-06-18T06:54:36.112506Z",
     "iopub.status.idle": "2022-06-18T06:54:36.117302Z",
     "shell.execute_reply": "2022-06-18T06:54:36.116288Z",
     "shell.execute_reply.started": "2022-06-18T06:54:36.11308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Peek at the values\n",
    "#display(df_train)\n",
    "#display(df_test)\n",
    "\n",
    "# Display information about dtypes and missing values\n",
    "#display(df_train.info())\n",
    "#display(df_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Baseline ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:36.119663Z",
     "iopub.status.busy": "2022-06-18T06:54:36.118984Z",
     "iopub.status.idle": "2022-06-18T06:54:36.130117Z",
     "shell.execute_reply": "2022-06-18T06:54:36.129Z",
     "shell.execute_reply.started": "2022-06-18T06:54:36.11962Z"
    }
   },
   "outputs": [],
   "source": [
    "models={}\n",
    "def score_dataset(X, y, models_to_check=[]):\n",
    "    # Label encoding for categoricals\n",
    "    #\n",
    "    # Label encoding is good for XGBoost and RandomForest, but one-hot\n",
    "    # would be better for models like Lasso or Ridge. The `cat.codes`\n",
    "    # attribute holds the category levels.\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "            X[colname] = X[colname].cat.codes\n",
    "        # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    log_y = np.log(y)\n",
    "    #Linear regressor\n",
    "    lin_reg = LinearRegression()\n",
    "    #Decision Tree Regressor\n",
    "    tree_reg = DecisionTreeRegressor()\n",
    "    #Random forest regressor\n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    models_to_check = [lin_reg, tree_reg, forest_reg]\n",
    "    model_scores = {}\n",
    "    \n",
    "    for model in models_to_check:\n",
    "        models[model] = cross_val_score(model, X, log_y,\n",
    "                                scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "\n",
    "        model_scores[model] = np.sqrt(-(models[model].mean())) #negatbive sign - for reasons refer the book pg:73\n",
    "    return model_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:36.165845Z",
     "iopub.status.busy": "2022-06-18T06:54:36.165221Z",
     "iopub.status.idle": "2022-06-18T06:54:39.154969Z",
     "shell.execute_reply": "2022-06-18T06:54:39.154178Z",
     "shell.execute_reply.started": "2022-06-18T06:54:36.165805Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score LinearRegression(): 0.15276 RMSLE\n",
      "Baseline score DecisionTreeRegressor(): 0.21460 RMSLE\n",
      "Baseline score RandomForestRegressor(): 0.13947 RMSLE\n"
     ]
    }
   ],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "baseline_scores = score_dataset(X, y)\n",
    "for model in baseline_scores:\n",
    "    print(f\"Baseline score {model}: {baseline_scores[model]:.5f} RMSLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 2 - Feature Utility Scores #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = df_train['Alley'].factorize()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:39.161589Z",
     "iopub.status.busy": "2022-06-18T06:54:39.159447Z",
     "iopub.status.idle": "2022-06-18T06:54:39.172316Z",
     "shell.execute_reply": "2022-06-18T06:54:39.171196Z",
     "shell.execute_reply.started": "2022-06-18T06:54:39.161544Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our feature scores again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\",500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:39.174273Z",
     "iopub.status.busy": "2022-06-18T06:54:39.173662Z",
     "iopub.status.idle": "2022-06-18T06:54:40.761048Z",
     "shell.execute_reply": "2022-06-18T06:54:40.760126Z",
     "shell.execute_reply.started": "2022-06-18T06:54:39.17423Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "#mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:40.763417Z",
     "iopub.status.busy": "2022-06-18T06:54:40.763101Z",
     "iopub.status.idle": "2022-06-18T06:54:40.767817Z",
     "shell.execute_reply": "2022-06-18T06:54:40.766995Z",
     "shell.execute_reply.started": "2022-06-18T06:54:40.76339Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_uninformative(df, mi_scores):\n",
    "    return df.loc[:, mi_scores > 0.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:40.769816Z",
     "iopub.status.busy": "2022-06-18T06:54:40.76941Z",
     "iopub.status.idle": "2022-06-18T06:54:43.765732Z",
     "shell.execute_reply": "2022-06-18T06:54:43.764904Z",
     "shell.execute_reply.started": "2022-06-18T06:54:40.769784Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "X = drop_uninformative(X, mi_scores)\n",
    "\n",
    "#score_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 3 - Create Features #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:43.772673Z",
     "iopub.status.busy": "2022-06-18T06:54:43.769662Z",
     "iopub.status.idle": "2022-06-18T06:54:43.779975Z",
     "shell.execute_reply": "2022-06-18T06:54:43.778845Z",
     "shell.execute_reply.started": "2022-06-18T06:54:43.772625Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    X = df.copy()\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Create Features with Pandas ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:43.78185Z",
     "iopub.status.busy": "2022-06-18T06:54:43.781075Z",
     "iopub.status.idle": "2022-06-18T06:54:43.794695Z",
     "shell.execute_reply": "2022-06-18T06:54:43.793932Z",
     "shell.execute_reply.started": "2022-06-18T06:54:43.781814Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def mathematical_transforms(df):\n",
    "    X = pd.DataFrame()  # dataframe to hold new features\n",
    "    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n",
    "    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n",
    "    # This feature ended up not helping performance\n",
    "    # X[\"TotalOutsideSF\"] = \\\n",
    "    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n",
    "    #     df.Threeseasonporch + df.ScreenPorch\n",
    "    return X\n",
    "\n",
    "\n",
    "def interactions(df):\n",
    "    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n",
    "    X = X.mul(df.GrLivArea, axis=0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def counts(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"PorchTypes\"] = df[[\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "        \"EnclosedPorch\",\n",
    "        \"Threeseasonporch\",\n",
    "        \"ScreenPorch\",\n",
    "    ]].gt(0.0).sum(axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def break_down(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_transforms(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## k-Means Clustering ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:43.796827Z",
     "iopub.status.busy": "2022-06-18T06:54:43.795989Z",
     "iopub.status.idle": "2022-06-18T06:54:43.811464Z",
     "shell.execute_reply": "2022-06-18T06:54:43.810746Z",
     "shell.execute_reply.started": "2022-06-18T06:54:43.796783Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster_features = [\n",
    "    \"LotArea\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"FirstFlrSF\",\n",
    "    \"SecondFlrSF\",\n",
    "    \"GrLivArea\",\n",
    "]\n",
    "\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def cluster_distance(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    # Label features and join to dataset\n",
    "    X_cd = pd.DataFrame(\n",
    "        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n",
    "    )\n",
    "    return X_cd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:43.81368Z",
     "iopub.status.busy": "2022-06-18T06:54:43.813004Z",
     "iopub.status.idle": "2022-06-18T06:54:43.827305Z",
     "shell.execute_reply": "2022-06-18T06:54:43.826554Z",
     "shell.execute_reply.started": "2022-06-18T06:54:43.813638Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:43.831385Z",
     "iopub.status.busy": "2022-06-18T06:54:43.830602Z",
     "iopub.status.idle": "2022-06-18T06:54:43.843733Z",
     "shell.execute_reply": "2022-06-18T06:54:43.84306Z",
     "shell.execute_reply.started": "2022-06-18T06:54:43.83134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pca_inspired(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n",
    "    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n",
    "    return X\n",
    "\n",
    "\n",
    "def pca_components(df, features):\n",
    "    X = df.loc[:, features]\n",
    "    _, X_pca, _ = apply_pca(X)\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "pca_features = [\n",
    "    \"GarageAre\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"GrLivArea\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### PCA Application - Indicate Outliers ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:45.000271Z",
     "iopub.status.busy": "2022-06-18T06:54:44.999802Z",
     "iopub.status.idle": "2022-06-18T06:54:45.00621Z",
     "shell.execute_reply": "2022-06-18T06:54:45.005185Z",
     "shell.execute_reply.started": "2022-06-18T06:54:45.000229Z"
    }
   },
   "outputs": [],
   "source": [
    "def indicate_outliers(df):\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n",
    "    return X_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:45.00776Z",
     "iopub.status.busy": "2022-06-18T06:54:45.007448Z",
     "iopub.status.idle": "2022-06-18T06:54:45.025116Z",
     "shell.execute_reply": "2022-06-18T06:54:45.024175Z",
     "shell.execute_reply.started": "2022-06-18T06:54:45.007732Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=5)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit_transform(self, X, y, cols):\n",
    "        self.fitted_encoders_ = []\n",
    "        self.cols_ = cols\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            #print(idx_encode,idx_train)\n",
    "            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        #print(X_encoded)\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Create Final Feature Set ##\n",
    "\n",
    "Now let's combine everything together. Putting the transformations into separate functions makes it easier to experiment with various combinations. The ones I left uncommented I found gave the best results. You should experiment with you own ideas though! Modify any of these transformations or come up with some of your own to add to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:54:45.027018Z",
     "iopub.status.busy": "2022-06-18T06:54:45.026258Z",
     "iopub.status.idle": "2022-06-18T06:54:50.134378Z",
     "shell.execute_reply": "2022-06-18T06:54:50.133634Z",
     "shell.execute_reply.started": "2022-06-18T06:54:45.026984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{LinearRegression(): 0.14448221586626417,\n",
       " DecisionTreeRegressor(): 0.2104785850555261,\n",
       " RandomForestRegressor(): 0.13729303305402982}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"SalePrice\")\n",
    "    mi_scores = make_mi_scores(X, y)\n",
    "\n",
    "    # Combine splits if test data is given\n",
    "    #\n",
    "    # If we're creating features for test set predictions, we should\n",
    "    # use all the data we have available. After creating our features,\n",
    "    # we'll recreate the splits.\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        X_test.pop(\"SalePrice\")\n",
    "        X = pd.concat([X, X_test])\n",
    "\n",
    "    #Mutual Information\n",
    "    X = drop_uninformative(X, mi_scores)\n",
    "\n",
    "    #Transformations\n",
    "    X = X.join(mathematical_transforms(X))\n",
    "    X = X.join(interactions(X))\n",
    "    X = X.join(counts(X))\n",
    "    # X = X.join(break_down(X))\n",
    "    X = X.join(group_transforms(X))\n",
    "\n",
    "    #Clustering\n",
    "    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n",
    "    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n",
    "\n",
    "    #PCA\n",
    "    X = X.join(pca_inspired(X))\n",
    "    # X = X.join(pca_components(X, pca_features))\n",
    "    # X = X.join(indicate_outliers(X))\n",
    "\n",
    "    X = label_encode(X)\n",
    "\n",
    "    # Reform splits\n",
    "    if df_test is not None:\n",
    "        X_test = X.loc[df_test.index, :]\n",
    "        X.drop(df_test.index, inplace=True)\n",
    "\n",
    "    #Target Encoder\n",
    "    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n",
    "    if df_test is not None:\n",
    "        X_test = X_test.join(encoder.transform(X_test))\n",
    "\n",
    "    if df_test is not None:\n",
    "        return X, X_test\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "df_train, df_test = load_data()\n",
    "X_train = create_features(df_train)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "score_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 87)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Hyperparameter Tuning #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{LinearRegression(): array([-0.01608613, -0.0272621 , -0.01869832, -0.01702415, -0.03761046]),\n",
       " DecisionTreeRegressor(): array([-0.03737792, -0.05345298, -0.04246863, -0.04528419, -0.05168727]),\n",
       " RandomForestRegressor(): array([-0.01678688, -0.02264272, -0.02166348, -0.01700808, -0.01916071]),\n",
       " LinearRegression(): array([-0.01567479, -0.02300435, -0.01877791, -0.01554973, -0.03136877]),\n",
       " DecisionTreeRegressor(): array([-0.04104372, -0.05696438, -0.04077337, -0.04050088, -0.04222381]),\n",
       " RandomForestRegressor(): array([-0.01591305, -0.02398586, -0.01948069, -0.01643456, -0.01843272])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "                   param_distributions={&#x27;max_features&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CE2BB0&gt;,\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CBDD30&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "                   param_distributions={&#x27;max_features&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CE2BB0&gt;,\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CBDD30&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CE2BB0>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000203E8CBDD30>},\n",
       "                   random_state=42, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "X_train = create_features(df_train)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "param_distribs = {\n",
    "    'n_estimators': randint(low = 1, high = 200),\n",
    "    'max_features': randint(low = 30, high = 70)\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(list(models.keys())[-1], param_distributions = param_distribs,\n",
    "                               n_iter = 10, cv = 5, scoring = 'neg_mean_squared_error',\n",
    "                               random_state = 42)\n",
    "rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28060.458659347692 {'max_features': 68, 'n_estimators': 180}\n",
      "30368.722829004982 {'max_features': 58, 'n_estimators': 15}\n",
      "28056.96698282614 {'max_features': 37, 'n_estimators': 189}\n",
      "28182.877308966235 {'max_features': 50, 'n_estimators': 103}\n",
      "28590.44710505743 {'max_features': 48, 'n_estimators': 75}\n",
      "28240.503973889186 {'max_features': 40, 'n_estimators': 88}\n",
      "28231.982245749903 {'max_features': 65, 'n_estimators': 104}\n",
      "28447.25923099116 {'max_features': 53, 'n_estimators': 131}\n",
      "29054.927694332717 {'max_features': 51, 'n_estimators': 53}\n",
      "28517.77386073401 {'max_features': 31, 'n_estimators': 88}\n"
     ]
    }
   ],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 37, 'n_estimators': 189}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T06:55:17.268417Z",
     "iopub.status.busy": "2022-06-18T06:55:17.266495Z",
     "iopub.status.idle": "2022-06-18T06:55:24.345354Z",
     "shell.execute_reply": "2022-06-18T06:55:24.344467Z",
     "shell.execute_reply.started": "2022-06-18T06:55:17.268375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = create_features(df_train, df_test)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "forest_reg = RandomForestRegressor(**rnd_search.best_params_)\n",
    "# XGB minimizes MSE, but competition loss is RMSLE\n",
    "# So, we need to log-transform y to train and exp-transform the predictions\n",
    "forest_reg.fit(X_train, np.log(y))\n",
    "predictions = np.exp(forest_reg.predict(X_test))\n",
    "\n",
    "output = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
